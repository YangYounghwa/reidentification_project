{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7251ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa0462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VisionAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Multi-Head Self-Attention layer for vision tasks.\n",
    "    This layer is a core component of Vision Transformers (ViT).\n",
    "\n",
    "    Args:\n",
    "        dim (int): The embedding dimension of the input tokens.\n",
    "        heads (int): The number of attention heads.\n",
    "        dim_head (int, optional): The dimension of each attention head.\n",
    "                                  Defaults to dim // heads.\n",
    "        dropout (float, optional): Dropout rate. Defaults to 0.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        # The scale factor is a crucial detail for stabilizing training.\n",
    "        # It's the inverse square root of the head dimension.\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x input shape: (batch_size, num_patches, dim)\n",
    "\n",
    "        # 1. Project input to Q, K, V\n",
    "        # Shape: (batch_size, num_patches, inner_dim * 3)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        # 2. Reshape Q, K, V for multi-head attention\n",
    "        # Change shape to: (batch_size, heads, num_patches, dim_head)\n",
    "        q, k, v = map(\n",
    "            lambda t: t.reshape(t.shape[0], t.shape[1], self.heads, -1).permute(0, 2, 1, 3),\n",
    "            qkv\n",
    "        )\n",
    "\n",
    "        # 3. Calculate scaled dot-product attention scores\n",
    "        # (q @ k.transpose) -> (b, h, n, d) @ (b, h, d, n) -> (b, h, n, n)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # 4. Apply softmax to get attention weights\n",
    "        attn_weights = self.softmax(dots)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 5. Apply attention weights to V (values)\n",
    "        # (attn_weights @ v) -> (b, h, n, n) @ (b, h, n, d) -> (b, h, n, d)\n",
    "        attended_values = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # 6. Concatenate heads and project output\n",
    "        # First, reshape to (b, n, h*d) where h*d = inner_dim\n",
    "        out = attended_values.permute(0, 2, 1, 3).reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        # Finally, project back to the original embedding dimension `dim`\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da251c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReIDAtten_v2(nn.Module):\n",
    "    '''\n",
    "    ReID Atten v2\n",
    "    Reduced backbone of YOLOv11 \n",
    "    Uses Attention Layer for head.\n",
    "    157,024 parameters. \n",
    "    '''\n",
    "    def __init__(self, yolo_weights='yolo11n.pt', emb_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        yolo_model = YOLO(yolo_weights)\n",
    "        self.backbone = nn.Sequential(*yolo_model.model.model[:5])\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.backbone_output_dim = self._get_feat_dim()\n",
    "        # Caveat : dim = dim_head = heads\n",
    "        self.attn = VisionAttentionLayer(\n",
    "            dim=self.backbone_output_dim, \n",
    "            heads=4, \n",
    "            dim_head=self.backbone_output_dim // 4)\n",
    "        self.embed = nn.Linear(self.backbone_output_dim, emb_dim)\n",
    "\n",
    "    def _get_feat_dim(self):\n",
    "        x = torch.zeros((1, 3, 256, 128))\n",
    "        with torch.no_grad():\n",
    "            x = self.backbone(x)\n",
    "            return x.shape[1]  # fix here\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)          # (B, C, H, W)\n",
    "\n",
    "\n",
    "        flat = x.flatten(2).transpose(1, 2)  # (B, H*W, C)\n",
    "        # print(\"input to atten:\", flat.shape)\n",
    "        att = self.attn(flat)              # (B, H*W, C)\n",
    "        # print(att.shape)\n",
    "        att = att.mean(dim=1) \n",
    "        # print(att.shape)            # (B, C)\n",
    "        embed = self.embed(att)             # (B, 128)\n",
    "        return nn.functional.normalize(embed, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ec53a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128])\n",
      "157024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "model = ReIDAtten_v2()\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(\"ReIDAttenv2_6000.pth\",map_location=torch.device('cpu')))\n",
    "\n",
    "dummy = torch.randn(2,3,256,128)\n",
    "out = model(dummy)  # shape: (2,128,16,8)\n",
    "print(out.shape)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dae6c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# model = YourModelClass().eval()\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "dummy_input = torch.randn(1,3,256,128)  # adjust shape\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96b9a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] XML file: d:\\Dropbox\\LEARN\\vision_project1\\reidentification_project\\model_construct_ipy\\optimization\\model.xml\n",
      "[ SUCCESS ] BIN file: d:\\Dropbox\\LEARN\\vision_project1\\reidentification_project\\model_construct_ipy\\optimization\\model.bin\n"
     ]
    }
   ],
   "source": [
    "!ovc model.onnx --compress_to_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca936cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "model = core.read_model(\"model.xml\")\n",
    "compiled_model = core.compile_model(model, \"CPU\")\n",
    "\n",
    "input_tensor = compiled_model.input(0)\n",
    "output_tensor = compiled_model.output(0)\n",
    "\n",
    "result = compiled_model([input_numpy])[output_tensor]\n",
    "\n",
    "image = cv2.imread(\"sample.jpg\")  # Provide any 224x224 image\n",
    "image = cv2.resize(image, (224, 224))\n",
    "image = image.transpose(2, 0, 1) / 255.0  # HWC â†’ CHW, normalize\n",
    "image = np.expand_dims(image.astype(np.float32), axis=0)\n",
    "\n",
    "# Inference\n",
    "result = compiled_model([image])[output_layer]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dashboard_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
