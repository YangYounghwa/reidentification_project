{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f73653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4388840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_rank1(features, ids):\n",
    "    sim_matrix = cosine_similarity(features.numpy())  # [N, N]\n",
    "    num_samples = len(ids)\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        query_id = ids[i]\n",
    "        sim_scores = sim_matrix[i]\n",
    "        sim_scores[i] = -1  # exclude self-match\n",
    "        top_match_idx = np.argmax(sim_scores)\n",
    "        top_match_id = ids[top_match_idx]\n",
    "        if top_match_id == query_id:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    print(f\"Rank-1 Accuracy: {acc*100:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7b12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ResizePad:\n",
    "    def __init__(self,size=(256,128),fill=0):\n",
    "        self.target_h, self.target_w = size\n",
    "        self.fill = fill\n",
    "    def __call__(self,img):\n",
    "        orig_w, orig_h = img.size\n",
    "        scale = min(self.target_w/orig_w, self.target_h/orig_h)\n",
    "        new_w, new_h = int(orig_w * scale),int(orig_h*scale)\n",
    "\n",
    "        img = img.resize((new_w,new_h), Image.BILINEAR)\n",
    "\n",
    "        new_img = Image.new(\"RGB\",(self.target_w,self.target_h),(self.fill,)*3)\n",
    "        paste_x = (self.target_w-new_w)//2\n",
    "        paste_y = (self.target_h-new_h)//2\n",
    "        new_img.paste(img,(paste_x,paste_y))\n",
    "\n",
    "        return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2b9215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderBasedReIDValidationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            ResizePad((256, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "        self.samples = []  # list of (image_path, person_id)\n",
    "\n",
    "        for pid in os.listdir(root_dir):\n",
    "            folder = os.path.join(root_dir, pid)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "\n",
    "            image_paths = glob.glob(os.path.join(folder, '*.png'))\n",
    "            for img_path in image_paths:\n",
    "                xml_path = img_path.replace('.png', '.xml')\n",
    "                if os.path.exists(xml_path):  # optional check\n",
    "                    self.samples.append((img_path, pid))  # folder name = pid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, pid = self.samples[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        tensor = self.transform(img)\n",
    "        return tensor, pid, img_path  # ⬅️ include image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457a4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    ids = []\n",
    "    paths = []  # ⬅️ Add this\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 2:\n",
    "                imgs, pids = batch\n",
    "                batch_paths = [None] * len(imgs)  # dummy placeholder\n",
    "            else:\n",
    "                imgs, pids, batch_paths = batch  # ⬅️ Expect 3 outputs\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            emb = model(imgs)  # shape [B, D]\n",
    "            features.append(emb.cpu())\n",
    "            ids.extend(pids)\n",
    "            paths.extend(batch_paths)  # ⬅️ Append actual paths\n",
    "\n",
    "    features = torch.cat(features)\n",
    "    return features, ids, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1795b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv11ReID(nn.Module):\n",
    "    def __init__(self, yolo_weights='yolo11n.pt', emb_dim=128):\n",
    "        super().__init__()\n",
    "        yolo_model = YOLO(yolo_weights)\n",
    "\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "          yolo_model.model.model[0],\n",
    "          yolo_model.model.model[1],\n",
    "          yolo_model.model.model[2],\n",
    "          yolo_model.model.model[3],\n",
    "          yolo_model.model.model[4],\n",
    "          yolo_model.model.model[5],\n",
    "          yolo_model.model.model[6],\n",
    "          yolo_model.model.model[7],\n",
    "          )\n",
    "\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(self._get_feat_dim(), emb_dim)\n",
    "\n",
    "    def _get_feat_dim(self):\n",
    "        x = torch.zeros((1, 3, 256, 128))\n",
    "        with torch.no_grad():\n",
    "            f = self.backbone(x)\n",
    "\n",
    "            \n",
    "            # f = self.pool(f).flatten(1)\n",
    "            return f.shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        f = self.pool(x).flatten(1)\n",
    "        pooled = self.pool(x).flatten(1)  # ✅ apply once\n",
    "        emb = self.fc(pooled)\n",
    "        return nn.functional.normalize(emb, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df266225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25148\\385811224.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model3 = torch.load(r\"..\\saved_models\\reid_model_full_v0.1.pth\",map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-1 Accuracy: 98.15%\n"
     ]
    }
   ],
   "source": [
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "val_dataset = FolderBasedReIDValidationDataset(r\"..\\data\\valid_dataset1\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# model = YOLOv11ReID('yolov8n.pt')  # or your trained one\n",
    "# model.load_state_dict(torch.load(r\"..\\saved_models\\reid_model_full_v0.1.pth\"))\n",
    "model3 = torch.load(r\"..\\saved_models\\reid_model_full_v0.1.pth\",map_location=torch.device('cpu'))\n",
    "\n",
    "model3.to(device)\n",
    "\n",
    "features, ids ,paths = compute_embeddings(model3, val_loader, device)\n",
    "evaluate_rank1(features, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f85fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_gaps(features, ids,show=False):\n",
    "    sim_matrix = cosine_similarity(features.numpy())  # [N, N]\n",
    "    N = len(ids)\n",
    "    gaps = []\n",
    "\n",
    "    for i in range(N):\n",
    "        query_id = ids[i]\n",
    "        sim_scores = sim_matrix[i]\n",
    "\n",
    "        # exclude self\n",
    "        sim_scores[i] = -np.inf\n",
    "\n",
    "        # positive scores (same ID, not self)\n",
    "        pos_mask = np.array([j != i and ids[j] == query_id for j in range(N)])\n",
    "        neg_mask = np.array([ids[j] != query_id for j in range(N)])\n",
    "\n",
    "        if not np.any(pos_mask) or not np.any(neg_mask):\n",
    "            continue  # skip if no pos/neg samples\n",
    "\n",
    "        best_pos = np.max(sim_scores[pos_mask])\n",
    "        best_neg = np.max(sim_scores[neg_mask])\n",
    "\n",
    "        gap = best_pos - best_neg\n",
    "        gaps.append(gap)\n",
    "\n",
    "    gaps = np.array(gaps)\n",
    "    print(f\"Avg similarity gap (pos - hardest neg): {np.mean(gaps):.4f}\")\n",
    "    print(f\"% queries where positive > negative: {(gaps > 0).mean()*100:.2f}%\")\n",
    "    if(show):\n",
    "        # Optional: visualize\n",
    "        plt.hist(gaps, bins=40, color='blue', edgecolor='black')\n",
    "        plt.title(\"Distribution of (best positive - hardest negative) similarity gaps\")\n",
    "        plt.xlabel(\"Similarity Gap\")\n",
    "        plt.ylabel(\"Number of queries\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12da47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gap_triplet(query_path, pos_path, neg_path, gap_score):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "    for ax, path, title in zip(\n",
    "        axes,\n",
    "        [query_path, pos_path, neg_path],\n",
    "        ['Query', 'Best Positive', 'Hardest Negative']\n",
    "    ):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    color = 'green' if gap_score > 0 else 'red'\n",
    "    fig.suptitle(f\"Gap (pos - neg): {gap_score:.3f}\", color=color)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d03ef11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gap_failures(features, ids, paths, max_samples=5, gap_threshold=0.05):\n",
    "    sim_matrix = cosine_similarity(features.numpy())\n",
    "    N = len(ids)\n",
    "    shown = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        query_id = ids[i]\n",
    "        sim_scores = sim_matrix[i].copy()\n",
    "        sim_scores[i] = -np.inf\n",
    "\n",
    "        pos_mask = np.array([j != i and ids[j] == query_id for j in range(N)])\n",
    "        neg_mask = np.array([ids[j] != query_id for j in range(N)])\n",
    "\n",
    "        if not np.any(pos_mask) or not np.any(neg_mask):\n",
    "            continue\n",
    "\n",
    "        best_pos_idx = np.argmax(sim_scores * pos_mask)\n",
    "        best_neg_idx = np.argmax(sim_scores * neg_mask)\n",
    "\n",
    "        best_pos_sim = sim_scores[best_pos_idx]\n",
    "        best_neg_sim = sim_scores[best_neg_idx]\n",
    "        gap = best_pos_sim - best_neg_sim\n",
    "\n",
    "        # Show only confusing or failing cases\n",
    "        if gap < gap_threshold:\n",
    "            show_gap_triplet(paths[i], paths[best_pos_idx], paths[best_neg_idx], gap)\n",
    "            shown += 1\n",
    "            if shown >= max_samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc1878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25148\\1047273152.py:17: RuntimeWarning: invalid value encountered in multiply\n",
      "  best_pos_idx = np.argmax(sim_scores * pos_mask)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25148\\1047273152.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "  best_neg_idx = np.argmax(sim_scores * neg_mask)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25148\\1047273152.py:22: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  gap = best_pos_sim - best_neg_sim\n"
     ]
    }
   ],
   "source": [
    "visualize_gap_failures(features, ids, paths, max_samples=5, gap_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "003264e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg similarity gap (pos - hardest neg): 0.1479\n",
      "% queries where positive > negative: 98.15%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m compute_similarity_gaps(features,ids,show=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mcompute_similarity_gaps\u001b[39m\u001b[34m(features, ids, show)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m% queries where positive > negative: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(gaps\u001b[38;5;250m \u001b[39m>\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m).mean()*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(show):\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Optional: visualize\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     plt.hist(gaps, bins=\u001b[32m40\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m, edgecolor=\u001b[33m'\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m     plt.title(\u001b[33m\"\u001b[39m\u001b[33mDistribution of (best positive - hardest negative) similarity gaps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mSimilarity Gap\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "compute_similarity_gaps(features,ids,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a4ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pic_tag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
